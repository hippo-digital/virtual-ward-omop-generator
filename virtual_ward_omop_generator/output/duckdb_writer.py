"""DuckDB writer for OMOP CDM data with analysis-ready views."""

from datetime import datetime
from typing import Dict, Any, Optional, List
import pandas as pd
import duckdb
import os
from pathlib import Path
from ..models.base import BaseWriter, ConfigurableComponent
from ..models.omop import OMOPTables


class DuckDBWriter(BaseWriter, ConfigurableComponent):
    """Writes OMOP data to DuckDB database with analysis-ready views."""
    
    def __init__(self, config: Dict[str, Any]) -> None:
        super().__init__(config)
        self.omop_tables = OMOPTables(config)
        self.connection: Optional[duckdb.DuckDBPyConnection] = None
        
    def write(self, data: Dict[str, pd.DataFrame], output_path: str) -> None:
        """Write data to DuckDB database.
        
        Args:
            data: Dictionary of table name to DataFrame
            output_path: Path to output DuckDB file
        """
        self.logger.info(f"Writing OMOP data to DuckDB: {output_path}")
        
        # Ensure output directory exists
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Remove existing database file if it exists
        if os.path.exists(output_path):
            os.remove(output_path)
            self.logger.info(f"Removed existing database: {output_path}")
        
        try:
            # Connect to DuckDB
            self.connection = duckdb.connect(output_path)
            self.logger.info("Connected to DuckDB database")
            
            # Create schema and tables
            self.create_schema()
            
            # Insert data
            self._insert_data(data)
            
            # Create analysis views
            self._create_analysis_views()
            
            # Optimize database performance
            self.optimize_database()
            
            # Log write statistics and database stats
            self._log_write_stats(data)
            db_stats = self.get_database_stats()
            if db_stats:
                self.logger.info(f"Database size: {db_stats.get('database_size_mb', 0):.2f} MB")
            
            self.logger.info("Successfully wrote data to DuckDB")
            
        except Exception as e:
            self.logger.error(f"Error writing to DuckDB: {e}")
            raise
        finally:
            if self.connection:
                self.connection.close()
                self.connection = None
    
    def create_schema(self) -> None:
        """
        Create OMOP CDM schema with proper data types and constraints.
        
        This uses the ddl files generated by https://github.com/OHDSI/CommonDataModel/
        for the duckdb dialect of CDM 5.4.
        """
        if not self.connection:
            raise RuntimeError("No active DuckDB connection")
            
        self.logger.info("Creating OMOP CDM schema")

        ddl_folder = Path(__file__).relative_to(Path.cwd()).parent
        ddl_file = ddl_folder / "OMOPCDM_duckdb_5.4_ddl.sql"
        
        table_ddl = ddl_file.read_text()
        self.connection.execute(table_ddl)

        ddl_file = ddl_folder / "OMOPCDM_duckdb_5.4_indices.sql"
        index_ddl = ddl_file.read_text()
        self.connection.execute(index_ddl)
        
        # Create any additional tables not in the ddl
        table_order = [
            'survey_conduct'
        ]
        
        for table_name in table_order:
            schema = self.omop_tables.get_schema(table_name)
            if schema:
                self._create_table(schema)
                self._create_indexes(schema)

        # Insert metadata record(s)
        self.connection.execute("""
            insert into vocabulary (vocabulary_id, vocabulary_name, vocabulary_version, vocabulary_concept_id) 
            values ('None', 'None', 1, 1)
        """)

        self._insert_concepts_and_vocabularies()

        self.connection.execute(f"""
           insert into cdm_source (
                cdm_source_name, cdm_source_abbreviation, cdm_holder, source_release_date,
                cdm_release_date, cdm_version, cdm_version_concept_id, vocabulary_version
            ) values (
                'data', 'data', 'placeholder', ?, 
                ?, '5.4', 1, 1
            )""", parameters=(datetime.now(), datetime.now()))
    
    def _insert_concepts_and_vocabularies(self):
        concepts = pd.read_csv(self.config['vocabulary']['concept_file'])
        
        # Insert DOMAINs
        domain_ids = concepts['domain_id'].unique()
        domains = pd.DataFrame({
            'domain_id': domain_ids,
            'domain_name': domain_ids,
            'domain_concept_id': range(1_000_000_000, 1_000_000_000 + len(domain_ids))
        })
        domains.to_sql("domain", self.connection, if_exists='append', index=False)

        # Insert VOCABULARYs
        vocab_ids = concepts['vocabulary_id'].unique()
        vocabs = pd.DataFrame({
            "vocabulary_id": vocab_ids,
            "vocabulary_name": vocab_ids,
            "vocabulary_concept_id": range(1_100_000_000, 1_100_000_000 + len(vocab_ids))
        })
        vocabs.to_sql("vocabulary", self.connection, if_exists='append', index=False)

        # Insert Concept Classes
        cc_ids = concepts['concept_class_id'].unique()
        cclasses = pd.DataFrame({
            'concept_class_id': cc_ids,
            'concept_class_name': cc_ids,
            'concept_class_concept_id': range(1_200_000_000, 1_200_000_000 + len(cc_ids))
        })
        cclasses.to_sql("concept_class", self.connection, index=False, if_exists='append')

        # Insert concepts
        concepts.to_sql("concept", self.connection, index=False, if_exists='append')

    def _create_table(self, schema) -> None:
        """Create a single table with proper data types.
        
        Args:
            schema: OMOPTableSchema object
        """
        # Map OMOP data types to DuckDB types
        type_mapping = {
            'INTEGER': 'INTEGER',
            'DOUBLE': 'DECIMAL(18,3)',
            'VARCHAR(50)': 'VARCHAR(50)',
            'VARCHAR(60)': 'VARCHAR(60)',
            'VARCHAR(255)': 'VARCHAR(255)',
            'VARCHAR(20)': 'VARCHAR(20)',
            'VARCHAR(10)': 'VARCHAR(10)',
            'TEXT': 'TEXT',
            'DATE': 'DATE',
            'TIMESTAMP': 'TIMESTAMP'
        }
        
        # Build column definitions
        column_defs = []
        for col_name, col_type in schema.columns.items():
            duckdb_type = type_mapping.get(col_type, col_type)
            column_def = f"{col_name} {duckdb_type}"
            
            # Add primary key constraint
            if schema.primary_key == col_name:
                column_def += " PRIMARY KEY"
            
            column_defs.append(column_def)
        
        # Create table SQL
        create_sql = f"""
        CREATE TABLE {schema.name} (
            {', '.join(column_defs)}
        )
        """
        
        self.connection.execute(create_sql)
        self.logger.debug(f"Created table: {schema.name}")
    
    def _create_indexes(self, schema) -> None:
        """Create indexes for common query patterns.
        
        Args:
            schema: OMOPTableSchema object
        """
        if not schema.indexes:
            return
            
        for index_col in schema.indexes:
            index_name = f"idx_{schema.name}_{index_col}"
            index_sql = f"CREATE INDEX {index_name} ON {schema.name} ({index_col})"
            
            try:
                self.connection.execute(index_sql)
                self.logger.debug(f"Created index: {index_name}")
            except Exception as e:
                self.logger.warning(f"Failed to create index {index_name}: {e}")
    
    def _insert_data(self, data: Dict[str, pd.DataFrame]) -> None:
        """Insert data into tables using efficient batch operations with transaction management.
        
        Args:
            data: Dictionary of table name to DataFrame
        """
        self.logger.info("Inserting data into tables")
        
        # Insert in dependency order to respect foreign keys
        table_order = [
            'person',
            'observation_period',
            'visit_occurrence',
            'visit_detail',
            'condition_occurrence',
            'drug_exposure',
            'procedure_occurrence',
            'device_exposure',
            'measurement',
            'observation',
            'death',
            'note',
            'note_nlp',
            'specimen',
            'fact_relationship',
            'location',
            'care_site',
            'provider',
            'payer_plan_period',
            'cost',
            'drug_era',
            'dose_era',
            'condition_era',
            'episode',
            'episode_event',
            'metadata',
            'cdm_source',
            'concept',
            'vocabulary',
            'domain',
            'concept_class',
            'concept_relationship',
            'relationship',
            'concept_synonym',
            'concept_ancestor',
            'source_to_concept_map',
            'drug_strength',
            'cohort',
            'cohort_definition'
        ]
        
        # Use transaction for data consistency
        try:
            self.connection.execute("BEGIN TRANSACTION")
            
            for table_name in table_order:
                if table_name in data and not data[table_name].empty:
                    self._insert_table_data(table_name, data[table_name])
            
            self.connection.execute("COMMIT")
            self.logger.info("Successfully committed all data insertions")
            
        except Exception as e:
            self.connection.execute("ROLLBACK")
            self.logger.error(f"Transaction rolled back due to error: {e}")
            raise
    
    def _insert_table_data(self, table_name: str, df: pd.DataFrame) -> None:
        """Insert data for a single table with memory-efficient batch processing.
        
        Args:
            table_name: Name of the table
            df: DataFrame containing the data
        """
        # Configuration for batch processing
        batch_size = self.get_config_value('output.batch_size', 10000)
        max_memory_mb = self.get_config_value('output.max_memory_mb', 500)
        
        try:
            # Calculate memory usage
            memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
            
            if memory_mb > max_memory_mb or len(df) > batch_size:
                # Process in batches for large datasets
                self._insert_table_data_batched(table_name, df, batch_size)
            else:
                # Single batch insertion for smaller datasets
                self._insert_single_batch(table_name, df)
                
        except Exception as e:
            self.logger.error(f"Error inserting data into {table_name}: {e}")
            raise
    
    def _insert_single_batch(self, table_name: str, df: pd.DataFrame) -> None:
        """Insert data in a single batch.
        
        Args:
            table_name: Name of the table
            df: DataFrame containing the data
        """
        # Filter DataFrame to only include OMOP schema columns
        df_filtered = self._filter_omop_columns(table_name, df)
        
        # Use DuckDB's efficient DataFrame insertion
        self.connection.register('temp_df', df_filtered)
        
        try:
            # Get column names from the filtered DataFrame
            columns = ', '.join(df_filtered.columns)
            
            # Insert data
            insert_sql = f"""
            INSERT INTO {table_name} ({columns})
            SELECT {columns} FROM temp_df
            """
            
            self.connection.execute(insert_sql)
            self.logger.info(f"Inserted {len(df_filtered)} records into {table_name}")
            
        finally:
            self.connection.unregister('temp_df')
    
    def _filter_omop_columns(self, table_name: str, df: pd.DataFrame) -> pd.DataFrame:
        """Filter DataFrame to only include columns that exist in the OMOP schema.
        
        Args:
            table_name: Name of the OMOP table
            df: DataFrame containing the data
            
        Returns:
            Filtered DataFrame with only OMOP schema columns
        """
        schema = self.omop_tables.get_schema(table_name)
        if not schema:
            self.logger.warning(f"No schema found for table {table_name}, returning original DataFrame")
            return df
        
        # Get valid OMOP columns for this table
        valid_columns = set(schema.columns.keys())
        
        # Get columns that exist in both the DataFrame and the schema
        df_columns = set(df.columns)
        columns_to_keep = list(valid_columns.intersection(df_columns))
        
        # Log any columns that will be dropped
        dropped_columns = df_columns - valid_columns
        if dropped_columns:
            self.logger.debug(f"Dropping non-OMOP columns from {table_name}: {dropped_columns}")
        
        # Return filtered DataFrame
        return df[columns_to_keep]
    
    def _insert_table_data_batched(self, table_name: str, df: pd.DataFrame, batch_size: int) -> None:
        """Insert data in batches for memory efficiency.
        
        Args:
            table_name: Name of the table
            df: DataFrame containing the data
            batch_size: Number of records per batch
        """
        total_rows = len(df)
        num_batches = (total_rows + batch_size - 1) // batch_size
        
        self.logger.info(f"Inserting {total_rows} records into {table_name} in {num_batches} batches")
        
        for batch_num in range(num_batches):
            start_idx = batch_num * batch_size
            end_idx = min((batch_num + 1) * batch_size, total_rows)
            
            batch_df = df.iloc[start_idx:end_idx].copy()
            
            # Filter batch DataFrame to only include OMOP schema columns
            batch_df_filtered = self._filter_omop_columns(table_name, batch_df)
            
            # Register batch DataFrame
            batch_name = f'temp_batch_{batch_num}'
            self.connection.register(batch_name, batch_df_filtered)
            
            try:
                # Get column names from the filtered DataFrame
                columns = ', '.join(batch_df_filtered.columns)
                
                # Insert batch
                insert_sql = f"""
                INSERT INTO {table_name} ({columns})
                SELECT {columns} FROM {batch_name}
                """
                
                self.connection.execute(insert_sql)
                
                if batch_num % 10 == 0 or batch_num == num_batches - 1:
                    self.logger.debug(f"Inserted batch {batch_num + 1}/{num_batches} ({len(batch_df)} records)")
                
            finally:
                self.connection.unregister(batch_name)
        
        self.logger.info(f"Completed batched insertion of {total_rows} records into {table_name}")
    
    def optimize_database(self) -> None:
        """Optimize database performance after data insertion."""
        if not self.connection:
            raise RuntimeError("No active DuckDB connection")
            
        self.logger.info("Optimizing database performance")
        
        try:
            # Analyze tables for query optimization
            self.connection.execute("ANALYZE")
            
            # Vacuum to reclaim space and optimize storage
            self.connection.execute("VACUUM")
            
            self.logger.info("Database optimization completed")
            
        except Exception as e:
            self.logger.warning(f"Database optimization failed: {e}")
    
    def get_database_stats(self) -> Dict[str, Any]:
        """Get database statistics and performance metrics.
        
        Returns:
            Dictionary containing database statistics
        """
        if not self.connection:
            raise RuntimeError("No active DuckDB connection")
            
        stats = {}
        
        try:
            # Get table row counts
            table_stats = self.connection.execute("""
                SELECT table_name, estimated_size 
                FROM duckdb_tables() 
                WHERE schema_name = 'main'
            """).fetchall()
            
            stats['table_counts'] = {table: size for table, size in table_stats}
            
            # Get database size
            db_size = self.connection.execute("SELECT database_size FROM pragma_database_size()").fetchone()
            if db_size:
                # Convert to numeric if it's a string with units
                size_str = str(db_size[0])
                if 'bytes' in size_str.lower():
                    size_bytes = int(size_str.lower().replace('bytes', '').strip())
                elif 'kb' in size_str.lower():
                    size_bytes = int(float(size_str.lower().replace('kb', '').strip()) * 1024)
                elif 'mb' in size_str.lower():
                    size_bytes = int(float(size_str.lower().replace('mb', '').strip()) * 1024 * 1024)
                else:
                    try:
                        size_bytes = int(float(size_str))
                    except (ValueError, TypeError):
                        size_bytes = 0
                
                stats['database_size_bytes'] = size_bytes
                stats['database_size_mb'] = size_bytes / 1024 / 1024
            
            # Get memory usage (use pragma_memory_usage instead of pragma_memory_limit)
            try:
                memory_info = self.connection.execute("SELECT * FROM pragma_memory_usage()").fetchone()
                if memory_info:
                    stats['memory_usage_bytes'] = memory_info[0]
                    stats['memory_usage_mb'] = memory_info[0] / 1024 / 1024
            except Exception as memory_error:
                # If memory usage pragma doesn't exist, skip it
                self.logger.debug(f"Memory usage stats not available: {memory_error}")
            
        except Exception as e:
            self.logger.warning(f"Failed to get database stats: {e}")
            
        return stats
    
    def _create_analysis_views(self) -> None:
        """Create analysis-ready views for PROM timeseries, device timeseries, and interventions."""
        self.logger.info("Creating analysis-ready views")
        
        # Create PROM timeseries view
        self._create_prom_timeseries_view()
        
        # Create device timeseries view
        self._create_device_timeseries_view()
        
        # Create interventions union view
        self._create_interventions_view()
        
        # Create episode summary view
        self._create_episode_summary_view()
        
        # Create additional analysis views
        self._create_patient_trajectory_view()
        self._create_intervention_timeline_view()
        self._create_ml_features_view()
    
    def _create_prom_timeseries_view(self) -> None:
        """Create view for patient-reported outcome measures timeseries."""
        prom_view_sql = """
        CREATE VIEW vw_prom_timeseries AS
        SELECT 
            o.person_id,
            e.episode_id,
            o.observation_datetime,
            o.observation_date,
            o.observation_concept_id,
            CASE 
                WHEN o.observation_concept_id = 2100000021 THEN 'Pain NRS'
                WHEN o.observation_concept_id = 2100000022 THEN 'Dyspnea'
                WHEN o.observation_concept_id = 2100000023 THEN 'Cough'
                WHEN o.observation_concept_id = 2100000024 THEN 'Sleep Quality'
                WHEN o.observation_concept_id = 2100000025 THEN 'Fatigue'
                WHEN o.observation_concept_id = 2100000026 THEN 'Appetite'
                WHEN o.observation_concept_id = 2100000027 THEN 'Dizziness'
                WHEN o.observation_concept_id = 2100000028 THEN 'Chest Pain'
                WHEN o.observation_concept_id = 2100000029 THEN 'Ankle Swelling'
                ELSE 'Unknown'
            END AS prom_type,
            o.value_as_number,
            o.value_as_concept_id,
            o.value_as_string,
            sc.survey_conduct_id,
            sc.survey_start_datetime,
            e.episode_start_datetime,
            e.episode_end_datetime,
            EXTRACT(EPOCH FROM (o.observation_datetime - e.episode_start_datetime)) / 3600.0 AS hours_from_episode_start
        FROM observation o
        INNER JOIN episode e ON o.person_id = e.person_id 
            AND o.observation_datetime >= e.episode_start_datetime 
            AND o.observation_datetime <= e.episode_end_datetime
        LEFT JOIN survey_conduct sc ON o.person_id = sc.person_id 
            AND DATE(o.observation_datetime) = DATE(sc.survey_start_datetime)
        WHERE o.observation_concept_id BETWEEN 2100000021 AND 2100000029
        ORDER BY o.person_id, e.episode_id, o.observation_datetime
        """
        
        self.connection.execute(prom_view_sql)
        self.logger.debug("Created vw_prom_timeseries view")
    
    def _create_device_timeseries_view(self) -> None:
        """Create view for device measurement timeseries."""
        device_view_sql = """
        CREATE VIEW vw_device_timeseries AS
        SELECT 
            m.person_id,
            e.episode_id,
            m.measurement_datetime,
            m.measurement_date,
            m.measurement_concept_id,
            CASE 
                WHEN m.measurement_concept_id = 3016502 THEN 'SpO2'
                WHEN m.measurement_concept_id = 3027018 THEN 'Heart Rate'
                WHEN m.measurement_concept_id = 3024171 THEN 'Respiratory Rate'
                WHEN m.measurement_concept_id = 3004249 THEN 'Systolic BP'
                WHEN m.measurement_concept_id = 3012888 THEN 'Diastolic BP'
                WHEN m.measurement_concept_id = 3020891 THEN 'Temperature'
                WHEN m.measurement_concept_id = 3025315 THEN 'Weight'
                ELSE 'Unknown'
            END AS measurement_type,
            m.value_as_number,
            m.unit_concept_id,
            m.range_low,
            m.range_high,
            e.episode_start_datetime,
            e.episode_end_datetime,
            EXTRACT(EPOCH FROM (m.measurement_datetime - e.episode_start_datetime)) / 3600.0 AS hours_from_episode_start,
            -- Flag measurements that are outside normal ranges
            CASE 
                WHEN m.range_low IS NOT NULL AND m.value_as_number < m.range_low THEN 'Below Normal'
                WHEN m.range_high IS NOT NULL AND m.value_as_number > m.range_high THEN 'Above Normal'
                ELSE 'Normal'
            END AS range_status
        FROM measurement m
        INNER JOIN episode e ON m.person_id = e.person_id 
            AND m.measurement_datetime >= e.episode_start_datetime 
            AND m.measurement_datetime <= e.episode_end_datetime
        WHERE m.measurement_concept_id IN (3016502, 3027018, 3024171, 3004249, 3012888, 3020891, 3025315)
        ORDER BY m.person_id, e.episode_id, m.measurement_datetime
        """
        
        self.connection.execute(device_view_sql)
        self.logger.debug("Created vw_device_timeseries view")
    
    def _create_interventions_view(self) -> None:
        """Create union view for all intervention types."""
        interventions_view_sql = """
        CREATE VIEW vw_interventions AS
        -- Drug exposures (medications)
        SELECT 
            de.person_id,
            e.episode_id,
            de.drug_exposure_start_datetime AS intervention_datetime,
            'Drug Exposure' AS intervention_category,
            CASE 
                WHEN de.drug_concept_id = 2100000041 THEN 'Antibiotic'
                WHEN de.drug_concept_id = 2100000042 THEN 'Diuretic'
                ELSE 'Other Medication'
            END AS intervention_type,
            de.drug_source_value AS intervention_description,
            de.days_supply,
            NULL AS procedure_duration_hours,
            e.episode_start_datetime,
            e.episode_end_datetime,
            EXTRACT(EPOCH FROM (de.drug_exposure_start_datetime - e.episode_start_datetime)) / 3600.0 AS hours_from_episode_start
        FROM drug_exposure de
        INNER JOIN episode e ON de.person_id = e.person_id 
            AND de.drug_exposure_start_datetime >= e.episode_start_datetime 
            AND de.drug_exposure_start_datetime <= e.episode_end_datetime
        
        UNION ALL
        
        -- Procedure occurrences
        SELECT 
            po.person_id,
            e.episode_id,
            po.procedure_datetime AS intervention_datetime,
            'Procedure' AS intervention_category,
            CASE 
                WHEN po.procedure_concept_id = 2100000051 THEN 'Urgent Review'
                WHEN po.procedure_concept_id = 2100000052 THEN 'ED Conveyance'
                WHEN po.procedure_concept_id = 2100000053 THEN 'Oxygen Therapy'
                WHEN po.procedure_concept_id = 2100000054 THEN 'Acute Admission'
                ELSE 'Other Procedure'
            END AS intervention_type,
            po.procedure_source_value AS intervention_description,
            NULL AS days_supply,
            EXTRACT(EPOCH FROM (po.procedure_end_datetime - po.procedure_datetime)) / 3600.0 AS procedure_duration_hours,
            e.episode_start_datetime,
            e.episode_end_datetime,
            EXTRACT(EPOCH FROM (po.procedure_datetime - e.episode_start_datetime)) / 3600.0 AS hours_from_episode_start
        FROM procedure_occurrence po
        INNER JOIN episode e ON po.person_id = e.person_id 
            AND po.procedure_datetime >= e.episode_start_datetime 
            AND po.procedure_datetime <= e.episode_end_datetime
        
        ORDER BY person_id, episode_id, intervention_datetime
        """
        
        self.connection.execute(interventions_view_sql)
        self.logger.debug("Created vw_interventions view")
    
    def _create_episode_summary_view(self) -> None:
        """Create episode summary view with key metrics."""
        episode_summary_sql = """
        CREATE VIEW vw_episode_summary AS
        SELECT 
            e.episode_id,
            e.person_id,
            e.episode_start_datetime,
            e.episode_end_datetime,
            EXTRACT(EPOCH FROM (e.episode_end_datetime - e.episode_start_datetime)) / (24.0 * 3600.0) AS episode_length_days,
            e.episode_source_value,
            
            -- Survey completion metrics
            COALESCE(survey_stats.total_surveys, 0) AS total_surveys,
            COALESCE(survey_stats.expected_surveys, 0) AS expected_surveys,
            CASE 
                WHEN COALESCE(survey_stats.expected_surveys, 0) > 0 
                THEN COALESCE(survey_stats.total_surveys, 0)::FLOAT / survey_stats.expected_surveys 
                ELSE 0.0 
            END AS survey_completion_rate,
            
            -- PROM metrics
            COALESCE(prom_stats.total_proms, 0) AS total_prom_observations,
            COALESCE(prom_stats.unique_prom_types, 0) AS unique_prom_types,
            
            -- Device measurement metrics
            COALESCE(device_stats.total_measurements, 0) AS total_device_measurements,
            COALESCE(device_stats.unique_measurement_types, 0) AS unique_measurement_types,
            
            -- Intervention metrics
            COALESCE(intervention_stats.total_interventions, 0) AS total_interventions,
            COALESCE(intervention_stats.drug_interventions, 0) AS drug_interventions,
            COALESCE(intervention_stats.procedure_interventions, 0) AS procedure_interventions,
            COALESCE(intervention_stats.urgent_reviews, 0) AS urgent_reviews,
            COALESCE(intervention_stats.ed_conveyances, 0) AS ed_conveyances
            
        FROM episode e
        
        -- Survey statistics
        LEFT JOIN (
            SELECT 
                e.episode_id,
                COUNT(DISTINCT sc.survey_conduct_id) AS total_surveys,
                EXTRACT(EPOCH FROM (e.episode_end_datetime - e.episode_start_datetime)) / (24.0 * 3600.0) AS expected_surveys
            FROM episode e
            LEFT JOIN survey_conduct sc ON e.person_id = sc.person_id 
                AND sc.survey_start_datetime >= e.episode_start_datetime 
                AND sc.survey_start_datetime <= e.episode_end_datetime
            GROUP BY e.episode_id, e.episode_start_datetime, e.episode_end_datetime
        ) survey_stats ON e.episode_id = survey_stats.episode_id
        
        -- PROM statistics
        LEFT JOIN (
            SELECT 
                e.episode_id,
                COUNT(*) AS total_proms,
                COUNT(DISTINCT o.observation_concept_id) AS unique_prom_types
            FROM episode e
            LEFT JOIN observation o ON e.person_id = o.person_id 
                AND o.observation_datetime >= e.episode_start_datetime 
                AND o.observation_datetime <= e.episode_end_datetime
                AND o.observation_concept_id BETWEEN 2100000021 AND 2100000029
            GROUP BY e.episode_id
        ) prom_stats ON e.episode_id = prom_stats.episode_id
        
        -- Device measurement statistics
        LEFT JOIN (
            SELECT 
                e.episode_id,
                COUNT(*) AS total_measurements,
                COUNT(DISTINCT m.measurement_concept_id) AS unique_measurement_types
            FROM episode e
            LEFT JOIN measurement m ON e.person_id = m.person_id 
                AND m.measurement_datetime >= e.episode_start_datetime 
                AND m.measurement_datetime <= e.episode_end_datetime
            GROUP BY e.episode_id
        ) device_stats ON e.episode_id = device_stats.episode_id
        
        -- Intervention statistics
        LEFT JOIN (
            SELECT 
                i.episode_id,
                COUNT(*) AS total_interventions,
                SUM(CASE WHEN i.intervention_category = 'Drug Exposure' THEN 1 ELSE 0 END) AS drug_interventions,
                SUM(CASE WHEN i.intervention_category = 'Procedure' THEN 1 ELSE 0 END) AS procedure_interventions,
                SUM(CASE WHEN i.intervention_type = 'Urgent Review' THEN 1 ELSE 0 END) AS urgent_reviews,
                SUM(CASE WHEN i.intervention_type = 'ED Conveyance' THEN 1 ELSE 0 END) AS ed_conveyances
            FROM vw_interventions i
            GROUP BY i.episode_id
        ) intervention_stats ON e.episode_id = intervention_stats.episode_id
        
        ORDER BY e.person_id, e.episode_start_datetime
        """
        
        self.connection.execute(episode_summary_sql)
        self.logger.debug("Created vw_episode_summary view")
    
    def _create_patient_trajectory_view(self) -> None:
        """Create view for patient trajectory analysis with archetype patterns."""
        trajectory_view_sql = """
        CREATE VIEW vw_patient_trajectories AS
        SELECT 
            e.person_id,
            e.episode_id,
            e.episode_start_datetime,
            e.episode_end_datetime,
            e.episode_length_days,
            
            -- Trajectory classification based on PROM patterns
            CASE 
                WHEN prom_variance.dyspnea_variance < 0.5 AND prom_variance.pain_variance < 0.5 THEN 'Stable'
                WHEN prom_variance.dyspnea_variance >= 2.0 OR prom_variance.pain_variance >= 2.0 THEN 'Flare-Severe'
                WHEN prom_variance.dyspnea_variance >= 1.0 OR prom_variance.pain_variance >= 1.0 THEN 'Flare-Moderate'
                WHEN prom_variance.dyspnea_variance >= 0.5 OR prom_variance.pain_variance >= 0.5 THEN 'Flare-Mild'
                ELSE 'Noisy Reporter'
            END AS trajectory_archetype,
            
            -- PROM trend indicators
            prom_trends.dyspnea_trend,
            prom_trends.pain_trend,
            prom_trends.fatigue_trend,
            
            -- Device measurement trends
            device_trends.spo2_trend,
            device_trends.hr_trend,
            device_trends.weight_trend,
            
            -- Intervention outcomes
            intervention_outcomes.first_intervention_hours,
            intervention_outcomes.total_interventions,
            intervention_outcomes.escalation_occurred
            
        FROM vw_episode_summary e
        
        -- PROM variance calculations
        LEFT JOIN (
            SELECT 
                episode_id,
                VARIANCE(CASE WHEN prom_type = 'Dyspnea' THEN value_as_number END) AS dyspnea_variance,
                VARIANCE(CASE WHEN prom_type = 'Pain NRS' THEN value_as_number END) AS pain_variance,
                VARIANCE(CASE WHEN prom_type = 'Fatigue' THEN value_as_number END) AS fatigue_variance
            FROM vw_prom_timeseries
            WHERE value_as_number IS NOT NULL
            GROUP BY episode_id
        ) prom_variance ON e.episode_id = prom_variance.episode_id
        
        -- PROM trend analysis (slope of values over time)
        LEFT JOIN (
            SELECT 
                episode_id,
                -- Simple trend calculation using first and last values
                CASE 
                    WHEN dyspnea_last > dyspnea_first THEN 'Worsening'
                    WHEN dyspnea_last < dyspnea_first THEN 'Improving'
                    ELSE 'Stable'
                END AS dyspnea_trend,
                CASE 
                    WHEN pain_last > pain_first THEN 'Worsening'
                    WHEN pain_last < pain_first THEN 'Improving'
                    ELSE 'Stable'
                END AS pain_trend,
                CASE 
                    WHEN fatigue_last > fatigue_first THEN 'Worsening'
                    WHEN fatigue_last < fatigue_first THEN 'Improving'
                    ELSE 'Stable'
                END AS fatigue_trend
            FROM (
                SELECT 
                    episode_id,
                    FIRST_VALUE(CASE WHEN prom_type = 'Dyspnea' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY observation_datetime) AS dyspnea_first,
                    LAST_VALUE(CASE WHEN prom_type = 'Dyspnea' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY observation_datetime ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS dyspnea_last,
                    FIRST_VALUE(CASE WHEN prom_type = 'Pain NRS' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY observation_datetime) AS pain_first,
                    LAST_VALUE(CASE WHEN prom_type = 'Pain NRS' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY observation_datetime ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS pain_last,
                    FIRST_VALUE(CASE WHEN prom_type = 'Fatigue' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY observation_datetime) AS fatigue_first,
                    LAST_VALUE(CASE WHEN prom_type = 'Fatigue' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY observation_datetime ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS fatigue_last
                FROM vw_prom_timeseries
                WHERE value_as_number IS NOT NULL
            ) trend_calc
            GROUP BY episode_id, dyspnea_first, dyspnea_last, pain_first, pain_last, fatigue_first, fatigue_last
        ) prom_trends ON e.episode_id = prom_trends.episode_id
        
        -- Device measurement trends
        LEFT JOIN (
            SELECT 
                episode_id,
                CASE 
                    WHEN spo2_last < spo2_first THEN 'Declining'
                    WHEN spo2_last > spo2_first THEN 'Improving'
                    ELSE 'Stable'
                END AS spo2_trend,
                CASE 
                    WHEN hr_last > hr_first THEN 'Increasing'
                    WHEN hr_last < hr_first THEN 'Decreasing'
                    ELSE 'Stable'
                END AS hr_trend,
                CASE 
                    WHEN weight_last > weight_first THEN 'Gaining'
                    WHEN weight_last < weight_first THEN 'Losing'
                    ELSE 'Stable'
                END AS weight_trend
            FROM (
                SELECT 
                    episode_id,
                    FIRST_VALUE(CASE WHEN measurement_type = 'SpO2' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY measurement_datetime) AS spo2_first,
                    LAST_VALUE(CASE WHEN measurement_type = 'SpO2' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY measurement_datetime ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS spo2_last,
                    FIRST_VALUE(CASE WHEN measurement_type = 'Heart Rate' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY measurement_datetime) AS hr_first,
                    LAST_VALUE(CASE WHEN measurement_type = 'Heart Rate' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY measurement_datetime ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS hr_last,
                    FIRST_VALUE(CASE WHEN measurement_type = 'Weight' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY measurement_datetime) AS weight_first,
                    LAST_VALUE(CASE WHEN measurement_type = 'Weight' THEN value_as_number END IGNORE NULLS) 
                        OVER (PARTITION BY episode_id ORDER BY measurement_datetime ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS weight_last
                FROM vw_device_timeseries
                WHERE value_as_number IS NOT NULL
            ) device_calc
            GROUP BY episode_id, spo2_first, spo2_last, hr_first, hr_last, weight_first, weight_last
        ) device_trends ON e.episode_id = device_trends.episode_id
        
        -- Intervention outcomes
        LEFT JOIN (
            SELECT 
                episode_id,
                MIN(hours_from_episode_start) AS first_intervention_hours,
                COUNT(*) AS total_interventions,
                CASE 
                    WHEN COUNT(CASE WHEN intervention_type = 'ED Conveyance' THEN 1 END) > 0 
                    OR COUNT(CASE WHEN intervention_type = 'Acute Admission' THEN 1 END) > 0 
                    THEN TRUE 
                    ELSE FALSE 
                END AS escalation_occurred
            FROM vw_interventions
            GROUP BY episode_id
        ) intervention_outcomes ON e.episode_id = intervention_outcomes.episode_id
        
        ORDER BY e.person_id, e.episode_start_datetime
        """
        
        self.connection.execute(trajectory_view_sql)
        self.logger.debug("Created vw_patient_trajectories view")
    
    def _create_intervention_timeline_view(self) -> None:
        """Create view for intervention timeline analysis."""
        timeline_view_sql = """
        CREATE VIEW vw_intervention_timeline AS
        SELECT 
            i.person_id,
            i.episode_id,
            i.intervention_datetime,
            i.intervention_type,
            i.hours_from_episode_start,
            
            -- Previous intervention context
            LAG(i.intervention_type) OVER (PARTITION BY i.episode_id ORDER BY i.intervention_datetime) AS previous_intervention,
            LAG(i.intervention_datetime) OVER (PARTITION BY i.episode_id ORDER BY i.intervention_datetime) AS previous_intervention_datetime,
            
            -- Time since last intervention
            EXTRACT(EPOCH FROM (i.intervention_datetime - LAG(i.intervention_datetime) OVER (PARTITION BY i.episode_id ORDER BY i.intervention_datetime))) / 3600.0 AS hours_since_last_intervention,
            
            -- Next intervention context
            LEAD(i.intervention_type) OVER (PARTITION BY i.episode_id ORDER BY i.intervention_datetime) AS next_intervention,
            LEAD(i.intervention_datetime) OVER (PARTITION BY i.episode_id ORDER BY i.intervention_datetime) AS next_intervention_datetime,
            
            -- Time to next intervention
            EXTRACT(EPOCH FROM (LEAD(i.intervention_datetime) OVER (PARTITION BY i.episode_id ORDER BY i.intervention_datetime) - i.intervention_datetime)) / 3600.0 AS hours_to_next_intervention,
            
            -- Escalation patterns
            CASE 
                WHEN i.intervention_type = 'Urgent Review' 
                AND LEAD(i.intervention_type) OVER (PARTITION BY i.episode_id ORDER BY i.intervention_datetime) = 'ED Conveyance'
                THEN 'Review_to_ED'
                WHEN i.intervention_type = 'ED Conveyance' 
                AND LEAD(i.intervention_type) OVER (PARTITION BY i.episode_id ORDER BY i.intervention_datetime) = 'Acute Admission'
                THEN 'ED_to_Admission'
                WHEN i.intervention_type = 'Urgent Review' 
                AND LEAD(i.intervention_type) OVER (PARTITION BY i.episode_id ORDER BY i.intervention_datetime) = 'Acute Admission'
                THEN 'Review_to_Admission'
                ELSE 'No_Escalation'
            END AS escalation_pattern,
            
            -- PROM context at time of intervention
            prom_context.dyspnea_value,
            prom_context.pain_value,
            prom_context.fatigue_value,
            
            -- Device context at time of intervention
            device_context.spo2_value,
            device_context.hr_value,
            device_context.weight_value
            
        FROM vw_interventions i
        
        -- PROM values around intervention time (within 6 hours)
        LEFT JOIN (
            SELECT 
                p.episode_id,
                p.observation_datetime,
                MAX(CASE WHEN p.prom_type = 'Dyspnea' THEN p.value_as_number END) AS dyspnea_value,
                MAX(CASE WHEN p.prom_type = 'Pain NRS' THEN p.value_as_number END) AS pain_value,
                MAX(CASE WHEN p.prom_type = 'Fatigue' THEN p.value_as_number END) AS fatigue_value
            FROM vw_prom_timeseries p
            WHERE p.value_as_number IS NOT NULL
            GROUP BY p.episode_id, p.observation_datetime
        ) prom_context ON i.episode_id = prom_context.episode_id 
            AND ABS(EXTRACT(EPOCH FROM (i.intervention_datetime - prom_context.observation_datetime)) / 3600.0) <= 6
        
        -- Device values around intervention time (within 6 hours)
        LEFT JOIN (
            SELECT 
                d.episode_id,
                d.measurement_datetime,
                MAX(CASE WHEN d.measurement_type = 'SpO2' THEN d.value_as_number END) AS spo2_value,
                MAX(CASE WHEN d.measurement_type = 'Heart Rate' THEN d.value_as_number END) AS hr_value,
                MAX(CASE WHEN d.measurement_type = 'Weight' THEN d.value_as_number END) AS weight_value
            FROM vw_device_timeseries d
            WHERE d.value_as_number IS NOT NULL
            GROUP BY d.episode_id, d.measurement_datetime
        ) device_context ON i.episode_id = device_context.episode_id 
            AND ABS(EXTRACT(EPOCH FROM (i.intervention_datetime - device_context.measurement_datetime)) / 3600.0) <= 6
        
        ORDER BY i.person_id, i.episode_id, i.intervention_datetime
        """
        
        self.connection.execute(timeline_view_sql)
        self.logger.debug("Created vw_intervention_timeline view")
    
    def _create_ml_features_view(self) -> None:
        """Create view with machine learning features for predictive modeling."""
        ml_features_sql = """
        CREATE VIEW vw_ml_features AS
        SELECT 
            e.episode_id,
            e.person_id,
            
            -- Episode characteristics
            e.episode_length_days,
            e.survey_completion_rate,
            
            -- PROM features (48-hour windows)
            prom_features.dyspnea_mean_48h,
            prom_features.dyspnea_max_48h,
            prom_trends.dyspnea_trend_48h,
            prom_features.pain_mean_48h,
            prom_features.pain_max_48h,
            prom_trends.pain_trend_48h,
            prom_features.fatigue_mean_48h,
            prom_features.fatigue_max_48h,
            prom_features.chest_pain_any_48h,
            prom_features.ankle_swelling_any_48h,
            
            -- Device measurement features (24-hour windows)
            device_features.spo2_min_24h,
            device_features.spo2_mean_24h,
            device_features.hr_max_24h,
            device_features.hr_mean_24h,
            weight_changes.weight_change_24h,
            device_features.bp_systolic_max_24h,
            
            -- Target variables (next 24 hours)
            targets.urgent_review_24h,
            targets.ed_conveyance_24h,
            targets.any_intervention_24h,
            targets.escalation_24h,
            
            -- Time-based features
            EXTRACT(DOW FROM e.episode_start_datetime) AS episode_start_dow,
            EXTRACT(HOUR FROM e.episode_start_datetime) AS episode_start_hour
            
        FROM vw_episode_summary e
        
        -- PROM features aggregated over 48-hour windows
        LEFT JOIN (
            SELECT 
                episode_id,
                AVG(CASE WHEN prom_type = 'Dyspnea' THEN value_as_number END) AS dyspnea_mean_48h,
                MAX(CASE WHEN prom_type = 'Dyspnea' THEN value_as_number END) AS dyspnea_max_48h,
                AVG(CASE WHEN prom_type = 'Pain NRS' THEN value_as_number END) AS pain_mean_48h,
                MAX(CASE WHEN prom_type = 'Pain NRS' THEN value_as_number END) AS pain_max_48h,
                AVG(CASE WHEN prom_type = 'Fatigue' THEN value_as_number END) AS fatigue_mean_48h,
                MAX(CASE WHEN prom_type = 'Fatigue' THEN value_as_number END) AS fatigue_max_48h,
                MAX(CASE WHEN prom_type = 'Chest Pain' AND value_as_number > 0 THEN 1 ELSE 0 END) AS chest_pain_any_48h,
                MAX(CASE WHEN prom_type = 'Ankle Swelling' AND value_as_number > 0 THEN 1 ELSE 0 END) AS ankle_swelling_any_48h
            FROM vw_prom_timeseries
            WHERE hours_from_episode_start <= 48
            AND value_as_number IS NOT NULL
            GROUP BY episode_id
        ) prom_features ON e.episode_id = prom_features.episode_id
        
        -- PROM trend features (separate subquery for trends)
        LEFT JOIN (
            SELECT 
                episode_id,
                -- Calculate trends using first and last values per episode
                (MAX(CASE WHEN prom_type = 'Dyspnea' AND rn_desc = 1 THEN value_as_number END) - 
                 MAX(CASE WHEN prom_type = 'Dyspnea' AND rn_asc = 1 THEN value_as_number END)) AS dyspnea_trend_48h,
                (MAX(CASE WHEN prom_type = 'Pain NRS' AND rn_desc = 1 THEN value_as_number END) - 
                 MAX(CASE WHEN prom_type = 'Pain NRS' AND rn_asc = 1 THEN value_as_number END)) AS pain_trend_48h
            FROM (
                SELECT 
                    episode_id,
                    prom_type,
                    value_as_number,
                    ROW_NUMBER() OVER (PARTITION BY episode_id, prom_type ORDER BY observation_datetime ASC) AS rn_asc,
                    ROW_NUMBER() OVER (PARTITION BY episode_id, prom_type ORDER BY observation_datetime DESC) AS rn_desc
                FROM vw_prom_timeseries
                WHERE hours_from_episode_start <= 48
                AND value_as_number IS NOT NULL
                AND prom_type IN ('Dyspnea', 'Pain NRS')
            ) ranked_proms
            WHERE rn_asc = 1 OR rn_desc = 1
            GROUP BY episode_id
        ) prom_trends ON e.episode_id = prom_trends.episode_id
        
        -- Device measurement features aggregated over 24-hour windows
        LEFT JOIN (
            SELECT 
                episode_id,
                MIN(CASE WHEN measurement_type = 'SpO2' THEN value_as_number END) AS spo2_min_24h,
                AVG(CASE WHEN measurement_type = 'SpO2' THEN value_as_number END) AS spo2_mean_24h,
                MAX(CASE WHEN measurement_type = 'Heart Rate' THEN value_as_number END) AS hr_max_24h,
                AVG(CASE WHEN measurement_type = 'Heart Rate' THEN value_as_number END) AS hr_mean_24h,
                MAX(CASE WHEN measurement_type = 'Systolic BP' THEN value_as_number END) AS bp_systolic_max_24h
            FROM vw_device_timeseries
            WHERE hours_from_episode_start <= 24
            AND value_as_number IS NOT NULL
            GROUP BY episode_id
        ) device_features ON e.episode_id = device_features.episode_id
        
        -- Weight change features (separate subquery)
        LEFT JOIN (
            SELECT 
                episode_id,
                (MAX(CASE WHEN rn_desc = 1 THEN value_as_number END) - 
                 MAX(CASE WHEN rn_asc = 1 THEN value_as_number END)) AS weight_change_24h
            FROM (
                SELECT 
                    episode_id,
                    value_as_number,
                    ROW_NUMBER() OVER (PARTITION BY episode_id ORDER BY measurement_datetime ASC) AS rn_asc,
                    ROW_NUMBER() OVER (PARTITION BY episode_id ORDER BY measurement_datetime DESC) AS rn_desc
                FROM vw_device_timeseries
                WHERE hours_from_episode_start <= 24
                AND measurement_type = 'Weight'
                AND value_as_number IS NOT NULL
            ) ranked_weights
            WHERE rn_asc = 1 OR rn_desc = 1
            GROUP BY episode_id
        ) weight_changes ON e.episode_id = weight_changes.episode_id
        
        -- Target variables (interventions in next 24 hours after 48h feature window)
        LEFT JOIN (
            SELECT 
                episode_id,
                MAX(CASE WHEN intervention_type = 'Urgent Review' 
                    AND hours_from_episode_start BETWEEN 48 AND 72 THEN 1 ELSE 0 END) AS urgent_review_24h,
                MAX(CASE WHEN intervention_type = 'ED Conveyance' 
                    AND hours_from_episode_start BETWEEN 48 AND 72 THEN 1 ELSE 0 END) AS ed_conveyance_24h,
                MAX(CASE WHEN hours_from_episode_start BETWEEN 48 AND 72 THEN 1 ELSE 0 END) AS any_intervention_24h,
                MAX(CASE WHEN intervention_type IN ('ED Conveyance', 'Acute Admission') 
                    AND hours_from_episode_start BETWEEN 48 AND 72 THEN 1 ELSE 0 END) AS escalation_24h
            FROM vw_interventions
            GROUP BY episode_id
        ) targets ON e.episode_id = targets.episode_id
        
        -- Only include episodes with sufficient length for feature extraction
        WHERE e.episode_length_days >= 3
        
        ORDER BY e.person_id, e.episode_start_datetime
        """
        
        self.connection.execute(ml_features_sql)
        self.logger.debug("Created vw_ml_features view")
    
    def get_analysis_queries(self) -> Dict[str, str]:
        """Get example analysis queries for the generated data.
        
        Returns:
            Dictionary of query name to SQL query
        """
        return {
            'prom_summary': """
                SELECT 
                    prom_type,
                    COUNT(*) as total_observations,
                    AVG(value_as_number) as mean_value,
                    STDDEV(value_as_number) as std_value,
                    MIN(value_as_number) as min_value,
                    MAX(value_as_number) as max_value
                FROM vw_prom_timeseries 
                WHERE value_as_number IS NOT NULL
                GROUP BY prom_type
                ORDER BY prom_type
            """,
            
            'device_summary': """
                SELECT 
                    measurement_type,
                    COUNT(*) as total_measurements,
                    AVG(value_as_number) as mean_value,
                    STDDEV(value_as_number) as std_value,
                    SUM(CASE WHEN range_status = 'Above Normal' THEN 1 ELSE 0 END) as above_normal_count,
                    SUM(CASE WHEN range_status = 'Below Normal' THEN 1 ELSE 0 END) as below_normal_count
                FROM vw_device_timeseries 
                WHERE value_as_number IS NOT NULL
                GROUP BY measurement_type
                ORDER BY measurement_type
            """,
            
            'intervention_summary': """
                SELECT 
                    intervention_type,
                    COUNT(*) as total_interventions,
                    COUNT(DISTINCT person_id) as unique_patients,
                    AVG(hours_from_episode_start) as avg_hours_from_start,
                    MIN(hours_from_episode_start) as min_hours_from_start,
                    MAX(hours_from_episode_start) as max_hours_from_start
                FROM vw_interventions
                GROUP BY intervention_type
                ORDER BY total_interventions DESC
            """,
            
            'episode_metrics': """
                SELECT 
                    COUNT(*) as total_episodes,
                    AVG(episode_length_days) as avg_episode_length,
                    AVG(survey_completion_rate) as avg_survey_completion,
                    AVG(total_interventions) as avg_interventions_per_episode,
                    SUM(CASE WHEN total_interventions > 0 THEN 1 ELSE 0 END)::FLOAT / COUNT(*) as episodes_with_interventions
                FROM vw_episode_summary
            """,
            
            'trajectory_analysis': """
                SELECT 
                    trajectory_archetype,
                    COUNT(*) as episode_count,
                    AVG(total_interventions) as avg_interventions,
                    AVG(first_intervention_hours) as avg_time_to_first_intervention,
                    SUM(CASE WHEN escalation_occurred THEN 1 ELSE 0 END)::FLOAT / COUNT(*) as escalation_rate
                FROM vw_patient_trajectories
                WHERE trajectory_archetype IS NOT NULL
                GROUP BY trajectory_archetype
                ORDER BY episode_count DESC
            """,
            
            'escalation_patterns': """
                SELECT 
                    escalation_pattern,
                    COUNT(*) as occurrence_count,
                    AVG(hours_to_next_intervention) as avg_escalation_time_hours,
                    AVG(dyspnea_value) as avg_dyspnea_at_intervention,
                    AVG(spo2_value) as avg_spo2_at_intervention
                FROM vw_intervention_timeline
                WHERE escalation_pattern != 'No_Escalation'
                GROUP BY escalation_pattern
                ORDER BY occurrence_count DESC
            """,
            
            'ml_feature_summary': """
                SELECT 
                    COUNT(*) as total_episodes_for_ml,
                    AVG(dyspnea_mean_48h) as avg_dyspnea_48h,
                    AVG(pain_mean_48h) as avg_pain_48h,
                    AVG(spo2_min_24h) as avg_spo2_min_24h,
                    SUM(urgent_review_24h)::FLOAT / COUNT(*) as urgent_review_rate,
                    SUM(ed_conveyance_24h)::FLOAT / COUNT(*) as ed_conveyance_rate,
                    SUM(any_intervention_24h)::FLOAT / COUNT(*) as any_intervention_rate
                FROM vw_ml_features
                WHERE dyspnea_mean_48h IS NOT NULL OR pain_mean_48h IS NOT NULL
            """
        }